% \begin{description}[leftmargin=-1\labelwidth]
% \item[\textsc{Overview}] \lipsum[1]
% \end{description}

\section{Introduction}
\label{sec:introduction}
The study of human behavior within a scientific, Darwinian framework is no longer an upstart enterprise, operating on the fringes of biology and several social sciences such as anthropology, psychology, and economics. Only anthropology, however, can rightly claim responsibility for introducing and elaborating the idea of the “transmission” of culture, and it is the only social science for which cultural transmission is—and has been, for more than a century—a central organizing concept for the discipline \citep{lyman2008cultural}. Over the past 40 years, intense interdisciplinary research in anthropology and related fields has yielded a large cohort of researchers pursuing a range of Darwinian based investigations into human behavior. This cohort includes those conducting studies using gene-culture coevolution (or “cultural transmission theory” more broadly) \citeeg{BR1985,CF1981,mesoudi2006towards,richerson2005not}, social psychological theories of norms and social epistemology \citeeg{binmore2005natural,fehr2004third,lewis1969convention}, and the study of strategic social interaction using the tools of evolutionary game theory \citeeg{gintis2000game,gintis2005moral,gintis2014bounds,weibull1997evolutionary}. Taken together, these approaches offer increasingly productive means for understanding the structure and patterns of human behavior in areas such as cumulative cultural evolution and the evolution of cooperation. 

These approaches to human behavior, however, tend to focus on the mechanisms by which information is passed and transformed between groups of people or from individual to individual.  In the context of historical science, these provide proximate explanations.   Tracing ultimate causation in order to account for why patterns of traits appear in varying frequencies through time and across space requires a way to document ``descent with modification.''  Evolutionary archaeology, in particular, seeks to document evolutionary change from the archaeological record by extracting data on the transmission and inheritance of cultural traits.  

For much of its history, anthropologists have traced and explained cultural patterns using intuitive, common-sensical methods \citeeg{Lyman:2009p24018,Lyman2000,lyman2000measuring,OBrien1999b}, which came to be partially systematized in the 1930's as ``culture history.'' \citep{lyman1997rise,lyman2001direct,Lyman2003a,lyman2008cultural,o1998james,o2000time}.  In the 1960's, a move to build explicit evolutionary explanations failed to capture the potential of the Darwinian paradigm, instead adopting a vitalistic and Lamarckian account of change \citep{Dunnell1980}.  The combination of intuitive methods combined with theoretical models that offered only generalizations about change limited the success of anthropological efforts to explain human behavior using an evolutionary framework for decades. 

Dunnell's influential works in the late 1970's and 1980s introduced a radical alternative to the ``cultural evolution'' then prevalent within archaeology \citep{Dunnell1978,dunnell1982harvey,Dunnell1980,Dunnell1989}.  These works ultimately have supplied the basis and rationale for at least three distinctive research programs that focus on cultural transmission, each concentrating on a different scale of analysis.  While there are other possible divisions between research programs, such as the focus on learning theory that comprises much of the work conducted in Japan \citeeg{8853,Aoki1987,aoki2011rates,Aoki2013Determinants-of,Aoki2015,Nakahashi2013Cultural-Evolut,Nishiaki2013Introduction,Terashima2013,Wakano:2007p8544,Wakano2004}, the division used here is focused upon issues of spatiotemporal scale and analytical level \emph{sensu} \citet{Dunnell1971}.  

The first program centers on an argument made by \citet{Dunnell1978} that links the intuitive foundation of ``culture history'' with the large-scale needs of an evolutionary archaeology.  This research area focuses on the separation of ``homologous'' similarity from similarity due to convergent adaptation in order to understand ancestor-descendant relationships.  Its most notable area of development has been the adoption and development of cladistic methods borrowed from biology \citep{borgerhoff2006cultural,lyman1997rise,Lyman2006a,o1999seriation,o2000time,o2001cladistics,o2003cladistics,o2003resolving,OBrian2000,prentiss2019cultural,PRENTISS201564,Temkin2007}.  This research program is ``macroevolutionary'' in flavor and combines methods from biogeography, demography, and the comparative method to understand large-scale evolutionary history.  

A second research program aims to use formal models of cultural transmission to explain the distributional characteristics of stylistic variation in artifact assemblages.  This approach is ``microevolutionary'' in scale since it focuses on model fitting and inference within single assemblages or small sets of assemblages, taken to represent a population.  The work started from Dunnell's \citeyearpar{Dunnell1978} in which differences were not subject to natural selection (i.e., ``neutral traits'') but also includes the pioneering modeling work of Boyd and Richerson \citeyearpar{BR1985} and Cavalli-Sforza and Feldman \citeyearpar{CF1981}.  Fraser Neiman \citeyearpar{Neiman1995} provided a quantitative basis for this effort using the Wright-Fisher model from theoretical population genetics to derive predictions about artifact class diversity measures. Using these predictions one can evaluate whether these measures meet the expectations of neutrality.  In  this way, Neiman provided a practical test for determining whether sets of classes used to describe assemblages displayed signs of neutrality and thus are usable for tracing homology and evolutionary theory \citep{eerkens2006cultural,Eerkens2007,Lipo1997,Lipo2001neutrality,Lipo2006}.  

The goals of this research program are several \citep{marwick2005can}.  First, in addition to statistical testing for goodness of fit to the expectations of neutrality, researchers examined the conditions that lead to neutrality and the potential for selective pressures to be involved in the evolution of cultural traits \citep{bettinger1999point,Bettinger2008,eerkens2005cultural,Evans2011,Pfeffer2001,steele2010ceramic,Wilhelmsen2001}.   Second, a large group of researchers have been interested in employing Boyd and Richerson's \citeyearpar{BR1985} models of transmission biases to characterize ways in which past populations may have had propensities for novelty-seeking, a bias towards conformity, or prestige-biased imitation  \citep{acerbi2014biases,Bentley2001,8913,Bentley2003,bentley2004random,Bentley2007b,bettinger1999point,Herzog2004,Kohler2004,Mesoudi2009,Shennan2001ceramic,shennan2008style}.  To the extent that this research tradition has made use of neutral models as a ``null hypothesis'' against which to test for departures, such studies have employed the same models and methods.  For example, both types of work have tended to focus on examination of frequency patterns within an assemblage or small set of assemblages \citep[see the detailed reviews by][]{kandler2019analysing,walsh2019introduction}.

A third research program is focused on scales in between single populations and the large-scale viewpoint of macroevolutionary studies.  Carl Lipo and myself, in collaboration with Dunnell prior to his passing in 2010, have been engaged in exploring ``mesoscale'' methods for tracing transmission patterns within and across regions, using observable units which incorporate time and change directly.  A focus above the ``microevolutionary,'' we believe, is essential given the diachronic and time averaged nature of the archaeological record.  Furthermore, the observable units we employ are critical to rendering our cultural transmission models empirically sufficient. Much of our work has involved extending classical seriation methods to be general purpose tools for constructing models of evolutionary histories \citep{Lipo1997,Lipo2001,Lipo2001neutrality,Lipo2005,Lipo2015,lipomadsen1997,lipomadsenhunt1995b,Madsen2008,Madsen2014,madsenlipo2015b}.  Others who have focused on the mesoscopic include \citet*{Kandler2013}'s important work on non-equilibrium neutral models, and Kandler's subsequent efforts that include approximate Bayesian generative approaches \citep{Kandler20150905,kandler2018generative,wilderkandler2015}.  

All three programs have had varying levels of success in the overall goal of using Darwinian evolutionary theory to account for cultural aspects of human behavior. The macroevolutionary research has demonstrated its utility for examining homology and tracing evolutionary relationships. The microevolutionary research program, on the other hand, has been struggling for the last decade to demonstrate its ability to fit detailed models of transmission bias to archaeological data \citep{kandler2019analysing}. Thus, much attention has been given to identifying sources of equifinality and seeking to remedy or “correct” for them \citep{barrett2019equifinality,premo2010equifinality}. In contrast, the mesoscopic work has sought to avoid problems of equifinality by focusing on scales of phenomena (i.e., multiple assemblages) that are more robust for statistical modeling \citeeg{Eerkens2007,Lipo2001a,o2015design}. 

The issue of equifinality remains a serious challenge to the goal of building a fully evolutionary archaeology.  While we can match data to the expectations we derive from models, how do we know that other matches are not likely?  Is it possible to ``correct'' for equifinality?  Do studies at scales above single assemblages offer the best solution to this problem?  

Based on the fundamental issue of equifinality, this dissertation has two parts.  First, I explore whether the challenges faced by the microevolutionary research program are solvable ones and whether there are fundamental limitations on our ability to distinguish the details of cultural transmission from the archaeological record.  Second, and following the mesoscopic approach, I investigate whether there are ways of constructing better observable units from our observations of the record that allow us to map homology and evolutionary history in empirically sufficient ways. These two questions have motivated my research for more than a decade, going back to joint work with Robert Dunnell, Carl Lipo, and Tim Hunt \citep{Lipo1997,lipomadsen1997,lipomadsenhunt1995b}.  

In the next section, I review the issues that have been discovered within the microevolutionary program in more detail.  I describe conclusions reached from several pieces of my own research, included in this dissertation that aimed at understanding the limitations of the dominant approach to detailed, microevolutionary modeling in archaeology. Those limitations largely stem from the conceptual approach that early attempts at microevolutionary modeling took in archaeology.  In particular, they are derived from the implicit willingness to ignore the mismatch between synchronic methods and modeling approaches, and the diachronic nature of the archaeological record.  As I show in one of the papers included here, we are unlikely to simply ``correct'' for the diachronic nature of the records simply by creating better methods.  Microevolutionary approaches simply require data that are not typically available in the archaeological record except in a few idiosyncratic cases \citep{scholnick2010apprenticeship,Mallios2014}.

The idea that the initial promise of the microevolutionary program may be limited by fundamental equifinality issues will be disappointing to some.  In particular, we may not be able to uniquely fit social psychological models of transmission to archaeological data, and this limits our ability to consider cultural transmission modeling as a bridge between social theory and most archaeological data.  My own contention is that we should not expect such a bridge to be possible, given the nature of the empirical record we study.  As Richerson and Boyd \citeyearpar{richerson2008response} note, the archaeological record ``speaks softly'' on too many of the pieces of information one needs in order to make the microevolutionary modeling approach more than an interpretive heuristic in most cases. 

At the other end of the spectrum, the macroevolutionary approach and cladistic modeling are important tools for mapping large-scale evolutionary history, but the method typically relies on presence and absence data for classes and types. This reliance means that macroevolutionary methods are limited in their ability to resolve detail in evolutionary history. To map homology and history at more detailed temporal and spatial scales, we need tools that can make use of our quantitative understanding of variation over time and space.

Instead, I argue that we need to focus our attention to studying cultural transmission at the ``mesoscale.'' By moving to a more mesoscopic scale, we gain substantial increases in empirical sufficiency, tools that inherently incorporate the diachronic nature of our data, and potentially better models that are designed to provide explanations at the same scales at which we actually can and do measure variation.

Based on this reasoning, the heart of my dissertation research addresses the means for constructing observable units from our observations of the record that allow us to map homology and evolutionary history in empirically sufficient ways. In a series of two studies, I explore how we can use data structures—seriations and dependency graphs—as the observational units or ``features'' (in data science terminology) for fitting models to archaeological data. This work combines aspects of ``feature engineering'' from machine learning and data science, where variables and data are combined in ways that provide the maximum ability to discriminate between hypotheses or models, and good old-fashioned unit construction using tools like seriation that have long history within our discipline.

\section{Attempts to Assess Equifinality in the Microevolutionary Program}

While early and paradigm setting efforts began in the late 1970's \citeeg{Dunnell1978}, the microevolutionary program was first established in the early work of Cavalli-Sforza and Feldman \citeyearpar{CF1981} and Boyd and Richerson \citeyearpar{BR1985}.  These contributions were followed by a series of works in the late 1980s and 1990's \citeeg{Dunnell1989,Neiman1990,Neiman1995}.  The greatest growth of microevolutionary approaches, however, occurred after 2000 \citeeg{eerkens2005cultural,Hamilton2009,Kandler2013,Kandler20150905,Jordan2003,Shennan2001ceramic,perreault2010mobility,scholnick2010apprenticeship,Rorabaugh2014,wilderkandler2015}.  Since that time, much attention has been given to the application of various statistical tests to a variety of data sets, both contemporary \citeeg{Herzog2004,8913}, and archaeological \citeeg{Jordan2003,mesoudi2008cultural,Shennan2001ceramic}.  The goal of these studies has been to demonstrate the potential to examine modes of transmission that appear at the scale of individuals.  Much of this work shares a common conceptual structure:

\begin{itemize}
    \item A chosen model of transmission bias (or models) is compared to a model lacking bias (typically, the Wright-Fisher model of genetic drift);
    \item Predictions for a diversity statistic or the shape of a frequency distribution are made from all of the models, sometimes using analytic equations (from Wright-Fisher), but more often by simulation;
    \item An empirical data set of artifact class frequencies are compared to model predictions to see which model has the closest match.
\end{itemize}

Most of the early studies presented their results as seemingly clear-cut and implied that it was possible to differentiate between models given existing, previously-collected data on artifact classes.  Within the last decade, however, the early clarity in the results has receded, especially after researchers began to reanalyze data sets using different approaches, with divergent results.

The European Neolithic Merzbach \emph{Linearbandkeramik} (LBK) ceramic dataset has often been held as an example of how it is possible to isolate microevolutionary mechanisms. Kandler and colleagues \citeyearpar{Kandler20150905}, however, note that after four studies previous to their own, the results remain conflicted as some studies support the hypothesis of neutrality for ceramic styles in the Merzbach LBK, while others reject neutrality in favor of anti-confirmist or novelty-seeking models of transmission.  These analyses included a variety of methods that include variants of the diversity index method \citep{Shennan2001ceramic,shennan2008style}, power law fits \citep{Bentley2003,shennan2008style}, Kandler's non-equilibrium assemblage comparison method \citep{Kandler2013}, and finally, an approximate Bayesian ``generative'' modeling approach \citep{Kandler20150905}.  Each analysis offered compelling evidence about microevolutionary mechanisms, though the results varied on a study-by-study basis.  

The failure to consistently replicate the initial conclusions drawn from the Merzbach LBK assemblages is not due to a faulty analytical method.  Rather, it is due to the inherent problem in seeking to isolate individual-scale mechanisms from these data.  In the past decade, this conclusion has become clearer when researchers began to focus on the sources of equifinality that might cause one to be unable to distinguish between biased and unbiased transmission models \citep{premo2010equifinality}.  Equifinality is a consideration whenever one seeks to account for samples of data from a complex empirical phenomenon, complete with chaos and nonlinearities \citep{bertalanffy1969general}.  When our theoretical models are relatively simple and stochastic in nature, it is likely that multiple models can generate the same outcomes.  

Equifinality has long been understood as an issue in archaeological interpretation and analysis \citeeg{gifford1991bones,kandler2018generative,lamberg1970excavations,Lyman2004,o1998basic,premo2010equifinality,rafferty2008time}.  It has not, however, been the subject of systematic study unlike disciplines such as geomorphology, climatology, and especially  hydrology  \citeeg{Culling1987,beven199612,Cicchetti:1996gp,Aronica1998,Savenije2001,beven2006manifesto,ebel2006physics,bonham2009nino,vrugt2009equifinality,cruslock2010geomorphologic,khatami2017equifinality,khatami2019equifinality}.  The lack of attention in archaeology on issues of equifinality has long thwarted progress towards consistent method development and cumulative knowledge generation.  And the problem is one that can be addressed:  given that varying factors can cause different transmission models to yield similar outcomes, it is incumbent upon us to design better analytical models and methods that are designed to circumvent the problem.  

The most readily apparent contributor to equifinality in our models is the mismatch between the synchronic structure of our models and predictions, and the diachronic, aggregate nature of the archaeological record. While many evolutionary models address the structure of variability generation at particular points in time, the data we evaluate represent a cumulative set of events of varying duration.  Thus, a key step in any evolutionary model is the derivation of model predictions for distributional characteristics or summary statistics that can be used to compare with our class frequency data. In most cases, however, the predictions or test statistics are synchronic; that is, they describe the situation that obtains in a hypothetical population subject to the transmission model at a point in time. Much of classical population genetic theory is structured to describe conditions at points in time.  For example, in pre-genomic population genetics researchers used stochastic models of genetic sampling within a population to produce predictions from the stationary distributions of the stochastic process and the extraction of marginal distributions or various statistics about the population or samples from the population \citeeg{ewens1972sampling,Ewens2004,slatkin1994exact,watterson1974sampling,watterson1978homozygosity}.\footnote{This style of modeling and analysis roughly characterizes ``pre-genomic'' theoretical population genetics; contemporary population genetics is considerably more diverse theoretically, especially after the introduction of the coalescent \citep{Wakeley2008} and the widespread use of phylogenetic methods on a flood of genetic data.}   Given the fact that the archaeological record represents the accumulation of events over time, naively borrowing this conceptual structure from population genetics without serious modification has been a major mistake. The lack of modifications to account for archaeological data is compounded by our inability to fully parameterize cultural transmission models, as Richerson and Boyd \citeyearpar[301-302]{richerson2008response} noted in their critique of microevolutionary efforts in archaeology. If our models rely on parameters such as population size but we cannot directly measure population using the archaeological record, we already working from a vastly weakened position. 

The archaeological record, however, is diachronic. As a result, the nature of the archaeological record has fundamental consequences for cultural transmission modeling. It is clear to all archaeologists that the archaeological record is not a sequence of ``moments in time'' but rather a cumulative record of artifact deposition whose temporal properties depend not just upon the intensity of use, but upon the sedimentary and geomorphological context \citep{schiffer1983toward,Schiffer1987,stein2001review,Stein1987,Stein1993,stein2001review,stein2003big}. As a result, there is a growing understanding that many, if not most, of our samples of the archaeological record reflect deposition over variable and significant spans of time. This fact means that archaeological data—counts and frequencies of artifact types, species in faunal assemblages and skeletal part inventories, paleobotanical assemblages, indeed, every kind of archaeological data—are potentially ``time-averaged'' \citep{walker1971significance}.  As a result, our data almost never refer to a specific configuration of a population, but are a kind of aggregate observation over a duration. 

The effects of “time averaging” have been studied in a variety of contexts within archaeology. The most prominent studies tend to be in Paleolithic deposits and  certain depositional contexts such as aeolian environments and surface contexts that are comprised of stable and old surface ages \citeeg{Bailey1981,bailey1983concepts,Bailey1987,bailey2007time,Bailey2008,Shott2008,Stern1994,stern2008time,Wandsnider2008}.  Similar to the attention paid by paleontologists and paleobiologists \citep{kidwell1997time,olszewski1997influence,Olszewski1999,olszewski2004modeling}, time averaging has also seen serious work in zooarchaeological and faunal analysis \citep{broughton1993diet,Grayson1998,lyman2003influence} given the importance of diversity indices and other summary statistics whose interpretation is greatly affected by assemblage duration.

My efforts to address the issue of equifinality are included in Chapter \ref{chap:timeaveraging-paper}. Written in 2012 and released on Arxiv.org, this article provided the first analysis of the effects of time averaged samples on the diversity statistics and statistical tests of neutrality that were commonly being employed in cultural transmission research within archaeology. I used agent-based simulation to sample the behavior of neutral and non-neutral transmission models under varying degrees of time averaging, and examined the resulting effects on common diversity statistics and neutrality tests to determine whether transmission bias may have affected the class frequencies we measure. My conclusion, which was then echoed by \citet{Premo2014}, is that even moderate amounts of temporal aggregation render standard "tests" for bias and neutrality unable to discriminate effectively between the two. Since the publication of this work, others have also taken the simulation approaches that Premo and I used. These more recent studies explore how time averaging in our data affects the spatial scale of cultural differentiation, and how the apparent rates of change we measure from archaeological samples scale with duration \citep{miller2018time,perreault2018time}. From the cumulative results of these works, it is quite clear that even moderate amounts of time averaging destroy the ability to treat archaeological samples ``as if'' they were synchronic.

This lesson led to a significant improvement in archaeological modeling of cultural transmission. In 2013, Kandler and Shennan \citeyearpar{Kandler2013} moved beyond synchronic model predictions and  instead demonstrated how it is possible to extract diachronic or “non-equilibrium” predictions about expected change over time from standard models of neutral and biased cultural transmission . Their work takes a diachronic approach to microevolution modeling rather than trying to “correct” a synchronic modeling approach to match the needs of the data.  The task of building on their foundational research will be vital to future success for those exploring cultural phenomena below the macroevolutionary level of analysis with archaeological data.

The work of Kandler and Shennan (2013) has been followed by a critique of, and replacement for, the way that archaeologists had been approaching the “model selection” step in the above conceptual approach. Crema \citeyearpar{Crema2014} as well as Kandler and colleagues \citeeg{Kandler20150905,wilderkandler2015,kandler2018generative,kandler2019analysing} have advocated for a “generative approach” to the study of  cultural transmission in which model selection is performed against empirical data. The generative approach combines approximate Bayesian model selection \citep{sisson2018handbook} with simulation modeling to produce predictive data sets.  Based in Bayesian methods, the power of this approach comes from one’s ability to estimate the “posterior distribution” of the statistical behaviors one can expect to see from each of a number of transmission models, along with an estimate of how likely each combination of observable statistics would be given the expectations of specific models. This approach allows one to rank statistics derived from archaeological data (e.g., a diversity measure, or the slope of a frequency distribution) by their likelihood to have arisen under each model. One can then examine the likelihoods presented by each model and determine whether there is a single model which could account for the observed data, or–more likely–whether there are still multiple models which could have generated the observed data.  Even more importantly, this combination of simulation and model selection allows the study of scenarios with non-stationary parameters, including growing or shrinking populations, and the incorporation of significant population structure in our models \citep{kandler2018generative,Rorabaugh2014}. 

This kind of model selection approach, which uses simulation from models to determine the likelihood of observed data under each model, is increasingly common across the sciences and occurs in a number of variants, from parametric and non-parametric bootstrapping \citep{efron1981nonparametric,efron1993introduction}, multiple model comparisons using a variety of information criteria \citep{burnham2002model}, posterior predictive simulation in Bayesian approaches \citep{gelman2013bayesian,gelman1996posterior,mcelreath2020statistical,robert1994bayesian}, and approximate Bayesian computation when the likelihood function cannot be evaluated or even formulated in a closed-form equation \citep{Beaumont2002,Toni2009,Beaumont2010,Csillery:2010jd,Marin2012,sisson2018handbook}.  These kinds of model selection approaches have demonstrated their value in evolutionary biology (see the excellent review by \citealp{brown2018modelperformance}), although as Brown and Thomson note, such techniques are not yet standard practice even given the mathematical sophistication of molecular phylogenetics and other evolutionary subfields.  Simulation-based model fitting should be widely applicable in archaeology.  In a particularly clear and sophisticated example, DiNapoli and colleagues \citeyearpar{dinapoli2019rapa} combined information-theoretic criteria and simulation from Poisson point-process models to explain the spatial pattern of \emph{ahu} on Rapa Nui, finding that their distribution is most strongly related to the distribution of sources of fresh water.  Since approaches like these allow us to quantify sources of uncertainty in our models and judge where models fit and also fail to match our data, they should become standard practice.  


\section{Are There Structural Equifinalities We Cannot Fix?}

Even with the power of generative modeling and simulation-based model selection, it has proven difficult to distinguish between neutral and biased models of transmission \citep{kandler2019analysing}. This difficulty arises from several sources of structural equifinality that make the archaeological fitting of detailed cultural transmission models to individual populations a difficult or even impossilbe enterprise in most circumstances.  One source of equifinality arises from the complex mixture of imitation, teaching, and mixtures of social and individual learning processes that we call ``transmission'' in real populations \citep{wimsatt2019articulating}.  Real populations of humans and social animals bear little resemblance to the pure populations of most models employed in the literature today.  Another source of equifinality arise from the stochastic nature of the models we necessarily employ and our inability to sample multiple realizations of a stochastic process when we try to fit individual assemblages to transmission models.  Any single data point may be compatible with a wide variety of models; only with multiple samples from the same realization of a process can we hope to do model selection with validity and statistical power.

The first source of structural equifinality is a consequence of the basic features of the phenomena we study: real human populations interact in complex ways. They never follow any single ``mode'' or strategy for adopting cultural information and learning from their peers. Realistic populations always include variation among individuals, and individuals often vary over time in the degree to which they vary the ways they learn or adopt behaviors . Individuals might follow conformist strategies at one point or novelty-seeking tendencies at another. Individuals vary the learning strategies they employ depending upon the type of situation faced, or the kind of trait involved. Thus, we should expect that populations will always be mixtures of cultural transmission modes and learning strategies, and one would expect the statistical signatures of these strategies to present complex statistical profiles. In the worst cases, the contributions of different learning strategies and biases may even ``cancel out'' at the population level, entirely eliminating our ability to distinguish one model from another. Given the effect of population mixtures, much of the equifinality we encounter in the microevolutionary approach is structural, and will not be resolved through better analytical methods—it is built into the phenomena themselves. 

In 2016, I wrote Chapter \ref{chap:ctmixtures-paper} to explore this issue. This chapter examines the degree to which we can distinguish mixed populations, using a variation on the generative approach described earlier. The study pairs agent-based simulations of differing population mixtures and compares them to each other and to a population of unbiased copiers. From each model in a pair, 23 different summary statistics are collected, all of which have been in use in the cultural transmission modeling literature. The simulations incorporates the effects of time averaging and sample size, to determine the interaction between these critical empirical factors and our ability to cleanly separate models from summary data. It accomplishes this task by generating several different sets of predicted data from each simulation pair with differing sample sizes and amounts of time averaging. I use a gradient boosting classifier to determine the degree to which any combination of summary statistics are able to distinguish between models in a comparison, and which observable variables are important for separating and identifying the models \citep{AlexeyNatekin2013,Friedman2000,hastie2009elements}.

In the chapter, I conclude that that equifinality is rife among mixture models. While the ability to census an entire population under conditions that lack time averaging permits model discrimination, sampling and time averaging quickly makes these discriminations statistically impossible. For example, it is typically impossible to distinguish a mixture of anti-conformists and conformists from a pure population of unbiased copiers.  The effects can simply ``cancel out'' at the level of the population.  Only under the simplest conditions in which observations are synchronic and populations are fully censused is there enough departure from the expectations of neutrality in a population that the classifier can find combinations of predictors that separate the distributions. But when one is limited to using finite samples and/or when samples represent significant intervals of time, it is difficult or impossible to tell which mixture of models may be represented in empirical data. Although the relationship between assemblage sample sizes and population sizes are typically not directly studied in most modeling exercises, our samples of the archaeological record are always samples of artifact discard and deposition from portions of a past population. This inherent sampling issue  and thus our inability to discriminate among mixtures of transmission modes seems structural and unavoidable.

It is not hard to understand why our simple models of transmission produce so many avenues for equifinality.  Our models of transmission are stochastic and incorporate chance in the processes of learning between individuals, and in the ways in which we model innovation. Chance is a key component of all historical phenomena. While the general claims of \citet{billiard2018stochasticity} about the lack consideration of this fact among archaeologists are accurate, most archaeologists accept that stochastic models are essential for modeling complex social behavior, and chance plays an important role in explaining any historical or evolutionary phenomenon. 

That said, the structure itself of our models of biased and unbiased transmission (especially for discrete traits) contribute strongly to the potential for equifinality.  At their core, each of the models we have attempted to fit to archaeological data are Markov chains that model trajectories of change in integer partitions \citep{crane2016ubiquitous}.  Our models are, structurally, all variations of sampling schemes from distributions within the Poisson-Dirichlet family; when the samples represent unordered partitions, the famous Ewens Sampling formula or distribution results.  The Ewens distribution does reflect the underlying probability model for the ``infinite-alleles'' model of neutral drift \citep{ewens1972sampling}, but there is strong convergence in distribution for other models as well \citep{huillet2007ewens}.  The Ewens distribution can represent the distribution of allelic partitions under selection as readily as it can neutrality \citep{gillespie1977sampling,grote2002approximate,khromov2018,sawyer1985sampling}.  

As a consequence of sharing this basic structure, it can be difficult to determine if a sample of data derives from any specific member of the family, especially at small sample sizes, and if the number of elements in a partition (artifact classes or categories, in our case) is small.  The small departures from a power law distribution, for example, that might be diagnostic in the context of a population census and using many classes (e.g., baby names or dog breed frequencies) are going to be difficult to detect with small samples and using small numbers of classes. The larger the number of partitions (or classes) represented and the larger the sample size used, the larger the number of states that can be empirically distinguished. Relatively speaking, small numbers of classes and small sample sizes lead to small numbers of distinguishable states. The problem we face, therefore, is that our microevolutionary models strongly overlap in their distributions of distinguishable states. The only potential detectable differences are slight variations in how probable any given state is from one model versus another, not its presence or absence in the solution set. 

This fact directly explains how the equifinality between cultural transmission models arises in the discrete case, especially when we only observe a single data point or realization.  Determining the best model fit, one typically needs multiple samples from the population under study so that one can quantitatively assess which model is the most likely fit. This step means some of the early studies that focused on the degree to which a single assemblage or several components from a single site compared to expectations of transmission models tended to have poorly defined results. While Kandler's \citeyearpar{Kandler2013} diachronic, non-equilibrium approach increases the statistical power of results by looking at the likelihood of trajectories of observations rather than single data points, this approach only becomes powerful in cases in which one can sample enough points through time and across space to manage complications of sample size and time averaging issues. 

Ulimtately, the combination of generative methods combined with greater attention to the size of the modeling state space  offers the only practical solution. Our models need to have richer state spaces, so that their predictions are not so strongly overlapping.  We need to develop predictions that encompass the quantitative aspects of information flow at more than just a couple of points in time and at more than one location. Richer predictions need to be matched by a richer set of observables that go beyond simple frequency arrays.  We need to develop structures that can represent diachronic change and spatial variation, as well as that vary in enough ways to be statistical distinguishable using typical archaeological sample sizes. Only by addressing these needs will we be able to get beyond structural equfinality in our modeling, and distinguish between hypotheses about evolutionary history in the archaeological record.

\section{Seriation and the Mesoscopic Approach to Cultural Transmission Modeling}

In 1995, Carl Lipo and I \citep{huntmadsenlipo1995a,lipomadsenhunt1995b,Lipo1997,Lipo2001neutrality,lipomadsen1997,Lipo2001a,Lipo2001} began to systematically exploring seriations as observable units for fitting transmission models to archaeological data.  We engaged in this exploration due to the recognition by \citet{Dunnell1970} that seriation automatically incorporates the diachronic nature of our data and includes finite durations for each of the assemblages that make up the seriation. In some of our early work on the subject \citeeg{Lipo1997}, we introduced an iterative method for finding deterministic solutions to the seriation problem, by partitioning the full set of assemblages into subsets, where each subset fully meets the requirements for unimodality.  

Rather than using seriation in its traditional format to build single linear orderings for all assemblages, this work involved creating multiple subsolutions.  Creating multiple solutions accomplishes two important results. First, it ensures that each subset meets the seriation criterion being used (e.g., unimodality or occurrence). Second, the creation of multiple solution directly incorporates the spatial variation present in the history of artifact classes across a region. This latter factor takes advantage of one of the key reasons that classical seriations used a “same local area” criterion to limit the amount of spatial variation one put into a seriation.  The spatial restriction is due to the fact that groups of assemblages in different places will produce different orders, and it is impossible to accommodate these different histories using a single linear order.  One way to account for the effect of space on the composition of class frequencies is simply to break the set of assemblages being ordered into the largest groups of different solutions. Each of these valid seriation solution tells one something unique about the history of artifact assemblages in specific places and times. In doing this kind of multiple solution technique, individual assemblages must be allowed to be included into solutions for multiple subsets. By examining how assemblages can fit into multiple seriation solutions provides a way to map how information may be differentially flowing in and out of localities within a region and through time. In practice, a small subset of assemblages do tend to occur in multiple subsets, as \citet{Lipo2001} found in his seriations of Mississippian ceramics from the central and lower Mississippi River  valley.  He also extended deterministic frequency seriation by calculating bootstrap confidence intervals around class frequencies, a step that allows one to assess the unimodality criterion for a set of assemblages while taking into account the likely effects of sampling error.  This allows one to \begin{dissparalist}
\item determine when two possible orderings for a set of assemblages cannot be distinguished, and thus certain assemblages are ``contemporaneous'' given the data we have
\item understand when small deviations from full unimodality are likely just vagaries of sampling error
\end{dissparalist}.

Much of this early work was still accomplished by manual assortment of assemblages and then software-based confirmation of the significance of candidate solutions via bootstrap analysis (using a Microsoft Excel macro package which still gets download requests today.\footnote{\url{http://www.evobeach.com/p/seriation.html}}  More recently, Lipo and I \citeyearpar{Lipo2015}, finishing work begun with Dunnell before his passing, automated the process of finding multiple-subset seriation solutions, by converting the problem to one of graph or tree construction. In that work, which is not included in this dissertation, we outlined an approach to seriation graph construction that employs the bootstrap confidence interval testing and employs heuristics to quickly prune the set of possible solutions. This approach helps avoid, but cannot prevent, the combinatorial explosion that occurs as the number of assemblages increases. In a short research paper, included here as Chapter \ref{chap:seriationcombinatorics-paper}, I examine how employing multiple solution groups affects the size of the “solution space” for the seriation problem and indeed increases the number of possible solutions for a given set of assemblages by orders of magnitude over the permutations available in a straight linear order.

Increasing the number of possible solutions might sound like a bad outcome, and without good heuristics on finding possible linkages within the candidate solutions it would be. But in practice, our “iterative deterministic seriation solution” (IDSS) method has proven relatively tractable with around a dozen assemblages, especially with the significant increases in computing power now available to researchers. But it is well worth looking at how to efficiently find solutions when we have 20, 30, or 50 assemblages. Why? Because the larger the set of assemblages we can include, with their variation in artifact class frequencies, the more data we are sampling from the single realization which was the actual history of cultural information in some span of time in a given region.

In order to improve our efficiency in finding complex solution graphs, I worked with Lipo to examine alternate criteria for forming solutions. There is nothing special about unimodality in a seriation. As \citet{Neiman1990,Neiman1995}  has documented, realizations of an unbiased transmission model like Wright-Fisher easily give rise to unimodal rises in the “popularity” of some trait, peak, and then decline, just as \citet{Nelson1916} and \citet{wissler1916application} described in the earliest recognizable “frequency seriations.” But transmission also gives rise to other types of patterns and can easily give rise to multimodal distributions for a trait over time as well (some things come back into “fashion”, as we well know from contemporary life). Based on this fact, it becomes clear that unimodality is not central to frequency seriation because it is the only pattern possible for transmission among a population of indivduals over time, but because it is a distinctive pattern among many that can be used to find the unique history of information sharing between and among communities. Recognizing this fact provides an opportunity to look for other ways of doing frequency seriation that yield equivalent results, but are more general and more efficient.

Chapter \ref{chap:multipleseriation-paper} describes our work comparing unimodality to other possible ordering algorithms, and in particular distance-minimization, building upon Kadane's \citeyearpar{Kadane1971} earlier work.  The principle we use in our reconceptualization of the seriation method is to find seriation solution graphs that globally minimize the total amount of change between neighboring pairs of assemblages. Because this method alludes to the ideas of mathematical “smoothness” and “continuity,” we dubbed the method “continuity seriation.” With continuity seriation, the efficiency of the calculation dramatically increases the size of data sets that can be analyzed, by providing a roughly 25x speedup in evaluating solutions (conservatively estimated). The expansion of the possible size of the solution space is a good thing when we think about cultural transmission models and their possible outcomes over many assemblages that span a region. If we treat a seriation solution for a set of assemblages as a realization of cultural transmission outcomes within a region over time, it is likely that some models of regional transmission are compatible with that realization (seriation) and that many will not be. By expanding the size of the potential solution space, we reduce the potential for equifinality.  

With continuity seriation available as a technique for obtaining solutions for larger numbers of assemblages at once, it becomes possible to tackle the question of whether seriations might be diagnostic of particular classes of evolutionary histories at the mesoscopic scale of analysis.  I examine this question in Chapter \ref{chap:computational-metapopulation}, by introducing a method of formalizing hypotheses about the regional structure of cultural transmission.  A transmission scenario is defined as a candidate regional history of the social network between communities and how it might change over time.  The results of repeated acts of cultural transmission over this evolving social network results in differential adoption and persistence of stylistic or neutral- behaving artifact classes (\emph{sensu} \citealt{Dunnell1978}) across the whole set. Standard social network models are typically synchronic snapshots, so the formalism adopted here is that of an ``interval temporal network'' to model how connectivity changes over time \citep{Holme2012}. 

In Chapter \ref{chap:computational-metapopulation}, I examine four different regional scenarios. These scenarios include configurations for \begin{dissparalist}
\item ``complete'' networks, where communities existing at any particular time index are all in communication
\item A ``nearest neighbor'' network in a long, thin configuration, where neighboring communities are connected to spatial neighbors along a river, with a few longer-distance connections (perhaps exogamous marriage relationships or trading relationships)
\item A ``nearest neighbor'' network in a compact, square configuration, where neighboring communities connect to each other in various directions, with a small number of longer-distance connections
\item A scenario in which a single regional population with complete network clusters of neighbors and a few longer-distance links between clusters splits partway through the time course of simulated evolution to form distinct ``lineages'' which are no longer in communication
\end{dissparalist}.

The goal of this study is to outline a potential method for \begin{dissparalist}
\item determining whether sets of transmission scenarios, expressed as interval temporal network models, can be distinguished in theory (i.e., are they equifinal)
\item can we use the statistical models we create in exploring the equifinality question to assess the ``fit'' of empirical datasets to our transmission scenarios
\end{dissparalist}?  To that end, I construct a simulation framework for sampling cultural trait frequencies across regional transmission scenarios, while applying realistic sampling and time averaging to the sampled data.  Our IDSS seriation method with continuity criterion then produces simulated seriations from replicated simulations from each regional transmission history.  I then consider how to best represent the structure and topology of seriation solutions in a statistical model.  In this work I employ the eigenvalue spectrum of the Laplacian matrix for the seriation graph, since the Laplacian is known to capture most of the topological information present in many classes of graphs.  A machine learning classifer model, trained on 90\% of the simulated seriation data, was employed to predict the most likely data generating process (transmission scenario) for a hold-out test set comprising the remaining 10\% of simulated seriations.  

In general, the results are promising. In the test set results, we can tell the difference between lineage splitting, nearest neighbor, and more homogeneous social networks.  The results indicate that we cannot distinguish between the two spatial configurations of nearest-neighbor scenarios, at least with the size of seriation solutions employed and with the Laplacian spectra as predictor variables.  This tells us something about the scale and level at which we can hope to frame mesoscopic models of cultural transmission.   Finally, I examined seriation results from the Late Prehistoric ceramic data in the central Mississippi River valley using a trained gradient boosting classifier.  The results from this analysis appears that the results are consistent with a lineage splitting event.  Overall, these results are consistent with previous conclusions reached using overall archaeological evidence for this data set \citep{Lipo2001}. This kind of generative approach with seriation graphs as the unit of observation has considerable promise, and this initial work is simply a down payment on exploring issues such as the sample sizes needed to resolve different classes of regional scenarios.


\section{Dependency Graphs and Incorporating Structured Information In Cultural Transmission Studies}

Much of this dissertation consists of my efforts to understand the flow of cultural information through space and time. One area that remains unexplored are the methods needed to understand how the \emph{content} of culturally transmitted information changes over time, and how the kind of information may affect its transmission. Simple cultural transmission models tend to treat cultural traits in the manner of ``bean bag genetics'' – as markers which come and go and are subject to innovation but have little structure among themselves. Cultural information and the skills that people inherit and pass on with that information are nothing like simple markers. Wimsatt \citep{wimsatt2007reproducing,wimsatt2013articulating,Wimsatt2014,wimsatt2019articulating} has made this the central focus of his work on cultural evolution, and has brought  together researchers with a variety of disciplinary foci to make development central to the study of cultural transmission. Within archaeology, Mesoudi and O’Brien \citeyear{mesoudi2008learning} explored structured relations between cultural traits.  \citet{tostevin2019content} has richly developed this idea and has combined work on trait structure with Wimsatt’s idea that some cultural traits provide ``scaffolding'' needed to learn others. He argues for the creation of ``thick descriptions'' that include the details of how social learning and cultural transmission articulate with the actual physical processes involved with technology. For example, he explores the relations between the physics of flintknapping and the processes of learning to flintknap and demonstrates that we can articulate the actual physics of the technology with the homologies we see over longer spans of time as methods are taught and learned.

The fruitful marriage of social learning theory, the details of specific technologies, and longer-term patterns in transmission is an exciting development in evolutionary archaeology. This avenue of research represents early steps in moving beyond simple models to explanatory models that use the cultural transmission framework to answer real questions about our evolutionary history. Successfully achieving this goal requires us to develop new tools required to make the articulation. These tools include the establishment of meaningful observable units and the determination of the statistical properties of those observables. 

The final chapter in this dissertation consists of my attempt to address these needs.  I wrote this chapter in 2015 for a volume on social learning in Neandertals and early modern humans. This volume focused on a ``learning hypothesis'' for behavioral modernity in the Upper Paleolithic  \citep{Nishiaki2013Introduction}.  In Chapter \ref{chap:semanticaxelrod-paper}, I examine a case of  ``structured information'' in which traits are modeled as having prerequisites. This situation often occurs in learning related to technology. For example, the acquisition of some skills may not be possible until we have mastered other skills. Conceptually, we can represent the relations among cultural traits or artifact classes using dependency trees. In these trees, nodes that are represented as higher in the tree are prerequisites for traits that are lower down.  Using this model of dependencies, I modeled how different learning models such as individual trial and error and targeted teaching by a peer produced cultural repertoires of different structure and richness. Like elsewhere in my dissertation, I employed a simulation approach and examined the ``knowledge graphs'' that simulated individuals have after many rounds of transmission while also conditioning each simulation run with different rates of teaching and individual innovation. Since the dependency structures and traits are abstract, the variables are trees of traits. I then analyzed the topology and symmetries of the trees of traits to determine their structure, using tools from algebraic graph theory \citep{godsil2001algebraic}. These structures varied from those that were deep and broad to those that were shallow and narrow. The results of my study supports the hypothesis that the evolution of mechanisms of teaching and apprenticeship would lead to enriched cultural repertoires and growth in cultural diversity.

Within the context of this dissertation, this study is significant because it demonstrates, once again, that studying cultural transmission within archaeology requires careful consideration of how we structure our observations. As \citet{Dunnell1971,dunnell1986methodological} repeatedly pointed out, the observational units we employ are not a given. The artifact class frequencies we construct for  one purpose do not necessarily mean they are informative for another purpose. We must build observational units using a combination of good old fashioned artifact classification as well as all of the mathematical, statistical, and machine learning sophistication we can muster. Only through this combination of efforts can we fruitfully use cultural transmission models in archaeology to tackle questions of evolutionary history. And we must tackle questions of evolutionary history if we are going to avail ourselves of the bodies of theoretical machinery, from evolutionary game theory to decision-theoretic modeling, to form complete evolutionary explanations \citeeg{gintis2014bounds}.
