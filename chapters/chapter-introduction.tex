% \begin{description}[leftmargin=-1\labelwidth]
% \item[\textsc{Overview}] \lipsum[1]
% \end{description}
\section{Introduction}
\label{sec:introduction}
The study of human behavior within a scientific, Darwinian framework is no longer an upstart enterprise, operating on the fringes of biology and several social sciences.  Intense interdisciplinary research has yielded a research framework that builds upon several pillars:  gene-culture coevolution (or ``cultural transmission theory'' more broadly), social psychological theories of  norms and social epistemology, and the study of strategic social interaction using the tools of evolutionary game theory \citep{gintis2014bounds}.  Taken together, this body of theory provides a framework for understanding the proximate causes and structure of phenomena such as cumulative cultural evolution and the evolution of cooperation.

Tracing ultimate causation using this framework of proximate causal theory is the role of the ``historical sciences.''  In the case of human evolution, anthropology and its subdisciplines serve this role, and in particular, archaeology and paleoanthropology.  Historical sciences like evolutionary archaeology engage in documenting the transmission and inheritance of cultural information, the extraction of evolutionary history from that historical record, and the testing of evolutionary models for explaining features of those evolutionary histories.  Anthropology can rightly claim a central role in introducing and elaborating the idea of the ``transmission'' of culture, and it is the only social science for which cultural transmission is---and has been, for more than a century---a \emph{central organizing concept} for the discipline \citep{lyman2008cultural}.  For much of our history, the tracing of cultural history, and thus evolution, was done using intuitive, common-sensical methods.   

But starting in the 1980's, Dunnell's suggestion that heritable variation could be mapped by tracking the spatiotemporal patterns of neutral or ``stylistic'' variation began a research tradition within archaeology aimed at documenting evolutionary trajectories (for a particularly good example, see \citealp{scholnick2010apprenticeship}).  Since then, evolutionary archaeologists have focused significant effort on development of methods and tools for tracing evolutionary trajectories \citep{Dunnell1978,Dunnell1980,Dunnell1992,lipo2006mapping,OBrian2000}.  These efforts have centered on exploration of cultural transmission models and methods for fitting such models to archaeological data.    

Dunnell's influential works led to several very different research programs focusing on cultural transmission models within archaeology, each concentrating on a different scale of analysis.\footnote{There are other possible divisions between research programs one can draw as well.  One important distinction is between the strong research program in Japan, which is much less focused on the Boyd and Richerson modeling framework than is Anglo-American anthropology, instead focusing on learning theory.  The split into research programs discussed here is focused entirely upon issues of spatiotemporal scale and analytical level.}  Dunnell  consistently emphasized the continuity between the foundational work in ``culture history'' and the needs of an evolutionary archaeology: we need to separate "homologous" similarity from similarity due to convergent adaptation in order to understand ancestor-descendant relationships.  At the largest scales, this led to the adoption and development of cladistic methods \citep{borgerhoff2006cultural,Lyman1997a,Lyman2006a,o1999seriation,o2000time,o2001cladistics,o2003cladistics,o2003resolving,OBrian2000,prentiss2019cultural,PRENTISS201564,Temkin2007}.  This research program is ``macroevolutionary'' in flavor, employing methods from biogeography, demography, and the comparative method to understand large-scale evolutionary history.  

A second research program aims to use formal models of cultural transmission to explain the distributional characteristics of stylistic variation in artifact assemblages, and is properly ``microevolutionary'' since it has focused on model fitting and inference within single assemblages or small sets of assemblages.  Starting from Dunnell's \citeyearpar{Dunnell1978} explicit linkage of culture-historical types with neutral traits that were not subject to natural selection, a vast literature has grown up around the pioneering modeling work of Boyd and Richerson \citeyearpar{BR1985} and Cavalli-Sforza and Feldman \citeyearpar{CF1981}.  Fraser Neiman \citeyearpar{Neiman1995} led this effort, using the Wright-Fisher model from theoretical population genetics to derive predictions about artifact class diversity measures that should hold true under neutrality, thus offering a practical way to test whether sets of classes used to describe assemblages displayed signs of neutrality and thus were usable for tracing homology and evolutionary history.  

The goals of this research program were several \citep{marwick2005can}.  First, researchers have been interested in determining whether natural selection could account for observed data, and thus there has been a strong focus on statistical testing for goodness of fit to the expectations of neutrality.  Rejection of neutrality would then open up research questions about the selective pressures involved in the evolution of kinds of cultural traits \citep{bettinger1999point,Bettinger2008,Eerkens2005,Evans2011,Pfeffer2001,steele2010ceramic,Wilhelmsen2001}.   Second, a large group of researchers have been interested in employing Boyd and Richerson's \citeyearpar{BR1985} models of transmission biases to characterize ways in which past populations may have had propensities for novelty-seeking, a bias towards conformity, or prestige-biased imitation \citep{acerbi2014biases,Bentley2001,8913,Bentley2003,8914,bentley2004random,Bentley2007b,bettinger1999point,Herzog2004,Kohler2004,mesoudi2009random,Shennan2001ceramic,shennan2008style}.  To the extent that this research tradition has made use of neutral models as a ``null hypothesis'' against which to test for departures, such studies have employed the same models and methods, and both types of work have tended to focus on examination of frequency patterns within an assemblage or small set of assemblages \citep[see the detailed reviews by][]{kandler2019analysing,walsh2019introduction}.

A third research program is focused on scales in between single populations and the large scale viewpoint of macroevolutionary studies.  Carl Lipo and myself, in collaboration with Dunnell prior to his passing in 2010, have been engaged in exploring ``mesoscopic'' methods for tracing transmission patterns within and across regions, using observable units which incorporate time and change directly.  A focus above the ``microevolutionary,'' we believe, is essential given the diachronic and time averaged nature of the archaeological record.  Furthermore, the observable units we employ are critical to rendering our cultural transmission models empirically sufficient. Much of our work involves extending classical seriation methods to be general purpose tools for constructing models of evolutionary histories \citep{Lipo1997,Lipo2001,Lipo2001neutrality,Lipo2005,Lipo2015,lipomadsen1997,lipomadsenhunt1995b,Madsen2008,Madsen2014,madsenlipo2015b}.  \citet*{Kandler2013}'s important work on non-equilibrium neutral models, and Kandler's subsequent work on approximate Bayesian generative approaches \citep{Kandler20150905,kandler2018generative,wilderkandler2015} is a good conceptual fit within a diachronic, mesoscopic research program as well.  

From today's vantage point, the macroevolutionary research program is useful and provides a strong set of methods for examining homology and evolutionary relationships at the largest scales, and at coarse grained levels within artifact classifications.  Cladistic methods have proven their worth in tracing evolutionary history at the largest scales, across taxa and applications \citeeg{Lyman:2006aa,OBrien2010,OBrien2003a,OBrien2001,o2008transmission,o2003resolving,o2002two,OBrien2015}.  The microevolutionary research program, on the other hand, has been struggling for the last decade to demonstrate the empirical sufficiency of its attempts to fit detailed models of transmission bias to archaeological data.  The broad community of cultural transmission researchers in archaeology have been focused on ways to identify the sources of equifinality and remedy or ``correct'' for them.  

The question I examine in this disseration has two parts:  First, are the equifinalities demonstrated within the microevolutionary research program solvable ones, or are there perhaps fundamental limitations on our ability to distinguish the details of cultural transmission from the archaeological record?  Second, are there ways of constructing better observable units from our observations of the record, which allow us to map homology and evolutionary history in empirically sufficient ways?  These two questions have motivated my research for more than a decade, going back to joint work with Robert Dunnell, Carl Lipo, and Tim Hunt \citep{Lipo1997,lipomadsen1997,lipomadsenhunt1995b}.  

In the next section, I review the issues that have been discovered within the microevolutionary program in more detail, and describe several pieces of my own research, included in this dissertation, aimed at understanding the limitations of the dominant approach to detailed, microevolutionary modeling in archaeology. Those limitations largely stem from the conceptual approach that early attempts at microevolutionary modeling took in archaeology, and in particular the willingness to ignore the mismatch between synchronic methods and modeling approaches, and the diachronic nature of the archaeological record.  But as I show in one of the papers included here, we are unlikely to simply ``correct'' for the diachronic nature of the records simply by creating better methods.  

The idea that the initial promise of the microevolutionary program may be limited by fundamental equifinality issues will be disappointing to some.  In particular, we may not be able to uniquely fit social psychological models of transmission to archaeological data, and this limits our ability to consider cultural transmission modeling an avenue for bridging social theory and most archaeological data.  My own contention is that we should not expect such a bridge to exist, given the nature of the empirical record we study.  As Richerson and Boyd \citeyearpar{richerson2008response} note, the archaeological record ``speaks softly'' on too many of the pieces of information one needs in order to make the microevolutionary modeling approach more than an interpretive heuristic in most cases. 

What we potentially gain by moving to a more ``mesoscopic,'' larger scaled approach will be better empirical sufficiency, tools that inherently incorporate the diachronic nature of our data, and potentially better models, which aim to provide explanations at the same scales at which we actually can and do measure variation.  The macroevolutionary approach and cladistic modeling are important tools for mapping large-scale evolutionary history, but the method typically relies on presence and absence data for classes and types, which means that macroevolutionary methods do not resolve detail in evolutionary history below some limit.  To map homology and history at more detailed temporal and spatial scales, we need tools that can make use of our quantitative understanding of variation over time and space.  

The heart of my dissertation research addresses the second part of the question I outlined above, and is discussed in the final section of this chapter.  In a series of two studies, I explore how we can use data structures---seriations and dependency graphs---as the observational units or ``features'' (in data science terminology) for fitting models to archaeological data.  This work combines aspects of ``feature engineering'' from machine learning and data science, where variables and data are combined in ways that provide the maximum ability to discriminate between hypotheses or models, and good old-fashioned ``unit construction'' using tools like seriation that have long history within our discipline.  

\section{Attempts to Assess Equifinality in the Microevolutionary Program}

The microevolutionary program really got underway after 2000, after some paradigm setting studies in the 1990's and early polemics \citep{Dunnell1978,Dunnell1989,Neiman1990,Neiman1995}.  Much of the earliest work focused on employing borrowed statistical tests to a variety of data sets, both contemporary \citeeg{Herzog2004,8913}, and archaeological \citeeg{Jordan2003,mesoudi2008cultural,Shennan2001}, to demonstrate the potential to examine modes of transmission.  Much of this work has a common conceptual structure:

\begin{itemize}
    \item A chosen model of transmission bias (or models) is compared to a model lacking bias (typically, the Wright-Fisher model of genetic drift);
    \item Predictions for a diversity statistic or the shape of a frequency distribution are made from all of the models, sometimes using analytic equations (from Wright-Fisher), but more often by simulation;
    \item An empirical data set of artifact class frequencies are compared to model predictions to see which model has the closest match.
\end{itemize}

Most of the early studies presented clear-cut results, seemingly demonstrating the ability to differentiate between models given existing, previously-collected data on artifact classes.  Within the last decade, however, that clarity has receded, especially after the same data set from the European Neolithic was analyzed using several different approaches, with divergent results. The use of the Merzbach \emph{Linearbandkeramik} ceramic dataset as an example in method-oriented papers has resulted in the de facto replication of research.  Kandler and colleagues \citeyearpar{Kandler20150905} note that after four studies previous to their own, we are still faced with conflicting results; some studies are consistent with a hypothesis of neutrality for ceramic styles in the Merzbach LBK, and others reject neutrality in favor of anti-conformist or novelty-seeking models of transmission.  It is clear that the equifinality seen in replicated studies of the Merzbach LBK assemblages is not due to a faulty analytical method, or indeed faulty methods at all.  The Merzbach assemblages were studied using variants of the diversity index method \citep{Shennan2001,shennan2008style}, power law fits \citep{Bentley2003,shennan2008style}, Kandler's non-equilibrium assemblage comparison method \citep{Kandler2013}, and finally, an approximate Bayesian ``generative'' modeling approach \citep{Kandler20150905}. 

Around 2010, a number of researchers began to focus on the sources of equifinality that might cause one to be unable to distinguish between biased and unbiased transmission models.  Equifinality will always occur when confronting samples of data from a complex empirical phenomenon, complete with chaos and nonlinearities, with relatively simple explanatory models \citep{bertalanffy1969general}.  Especially when models are stochastic in nature, there will always be multiple theoretical models which can generate the same outcomes.  Equifinality has long been understood as an issue in archaeological interpretation and analysis \citeeg{gifford1991bones,kandler2018generative,lamberg1970excavations,Lyman2004,o1998basic,premo2010equifinality,rafferty2008time}, but it is in disciplines like geomorphology, climatology, and especially in hydrology where equifinality has been systematically examined \citeeg{Culling1987,beven199612,Cicchetti:1996gp,Aronica1998,Savenije2001,beven2006manifesto,ebel2006physics,bonham2009nino,vrugt2009equifinality,cruslock2010geomorphologic,khatami2017equifinality,khatami2019equifinality}.  It seemed entirely possible that by considering the factors which could cause different transmission models to yield similar outcomes, we could design better analytical models and methods, to circumvent or correct the problem.

The most readily apparent factor is the mismatch between the synchronic structure of our models and predictions, and the diachronic, aggregate nature of the archaeological record.  In the conceptual schema outlined above, a key step is the derivation of model predictions for distributional characteristics or summary statistics, to compare with our class frequency data.  In most cases, the predictions or test statistics are \emph{synchronic}; that is, they describe the situation that obtains in a hypothetical population subject to the transmission model \emph{at a point in time}.  Much of classical population genetic theory is structured this way:  stochastic models of genetic sampling within a population are set up, and predictions from the model are made by analyzing the stationary distribution of the stochastic process and extracting marginal distributions or various statistics about the population or samples from the population \citeeg{ewens1972sampling,Ewens2004,slatkin1994exact,watterson1974sampling,watterson1978homozygosity}.\footnote{This style of modeling and analysis roughly characterizes ``pre-genomic'' theoretical population genetics; contemporary population genetics is considerably more diverse theoretically, especially after the introduction of the coalescent \citep{Wakeley2008} and the widespread use of phylogenetic methods on a flood of genetic data.}  Borrowing this conceptual structure from population genetics without serious modification has been a major mistake, especially given our inability to fully parameterize cultural transmission models with archaeological data, as Richerson and Boyd \citeyearpar[301-302]{Boyd2008} noted in their critique of microevolutionary efforts in archaeology.  

The diachronic nature of the archaeological record has more fundamental consequences for cultural transmission modeling than the difficulty in measuring population size, however.   It is clear to all archaeologists that the archaeological record is not a sequence of ``moments in time'' but rather an accumulative record of artifact deposition whose temporal properties depend not just upon the intensity of use, but upon the sedimentary and geomorphological context \citep{schiffer1983toward,Schiffer1987,stein2001review,Stein1987,Stein1993,stein2001review,stein2003big}.  As a result, there is a growing understanding that many, if not most, of our samples of the archaeological record reflect deposition over variable and and significant spans of time.  This means that archaeological data---counts and frequencies of artifact types, species in faunal assemblages and skeletal part inventories, paleobotanical assemblages, indeed, every kind of archaeological data---are potentially ``time averaged'' and refer not to a specific configuration of a population, but are a kind of aggregate observation over a duration.  
The effects of ``time averaging'' have been studied in a variety of contexts within archaeology, especially in Paleolithic studies, certain depositional contexts such as aeolian environments and surface contexts with stable and old surface ages \citeeg{Bailey1981,bailey1983concepts,Bailey1987,Bailey2007,Bailey2008,Shott2008,Stern1994,stern2008time,Wandsnider2008}.  Time averaging has also seen serious work in zooarchaeological and faunal analysis \citep{broughton1993diet,Grayson1998,Lyman2003} given the importance of diversity indices and other summary statistics whose interpretation is greatly affected by assemblage duration.\footnote{This work also draws upon a keen awareness of such issues in paleontology and paleobiology, for whom time averaging is ever-present \citep{kidwell1997time,olszewski1997influence,Olszewski1999,olszewski2004modeling}.}  

In Chapter \textcolor{red}{XX}, written in 2012 and released on Arxiv.org, I provided the first analysis of the effects of time averaged samples on the diversity statistics and statistical tests of neutrality that were commonly being employed in cultural transmission research within archaeology.  I use agent-based simulation to sample the behavior of neutral and non-neutral transmission models under varying degrees of time averaging, and examine the resulting effects on common diversity statistics and neutrality tests to determine whether transmission bias may have affected the class frequencies we measure.  The conclusion, which is echoed by Premo \citeyearpar{Premo2014}, is that even moderate amounts of temporal aggregation render standard "tests" for bias and neutrality unable to discriminate effectively between the two.  Since this work, the simulation approaches Premo and I used have been employed to good effect, to understand how time averaged data affects the spatial scale of cultural differentiation, and how the apparent rates of change we measure from archaeological samples scale with duration\citep{miller2018time,perreault2018time}.  It is quite clear that even moderate amounts of time averaging destroy the ability to treat archaeological samples ``as if'' they were synchronic.  

This lesson led to a significant improvement in archaeological modeling of cultural transmission.  Kandler, in joint work with Shennan, broke from the use of synchronic model predictions and extracted diachronic or ``non-equilibrium'' predictions about expected change over time from standard models of neutral and biased cultural transmission \citep{Kandler2013}.  This is the first work in the microevolutionary program that takes a diachronic approach to the core modeling task itself, rather than trying to ``correct'' the model to match the needs of the data, and will be foundational to future work below the macroevolutionary level of analysis.  

This key work was followed by a critique of, and replacement for, the way that archaeologists had been approaching the ``model selection'' step in the above conceptual approach.  Following initial work by Crema \citeyearpar{Crema2014} and colleagues, Kandler and colleagues introduced a ``generative approach'' to studying the behavior of cultural transmission models, and performing model selection against empirical data \citep{Kandler20150905,wilderkandler2015,kandler2018generative,kandler2019analysing}.  The generative approach combines approximate Bayesian model selection \citep{sisson2018handbook} with simulation modeling of often-complex cultural transmission scenarios.   The power of this approach comes in allowing the researcher to estimate the ``posterior distribution'' of statistical behaviors one can expect to see from each of a number of transmission models, along with an estimate of how likely each combination of observable statistics would be, under specific models.  This allows one to rank statistics derived from archaeological data (e.g., a diversity measure, or the slope of a frequency distribution) by their likelihood to have arisen under each model.  One can then examine the likelihoods presented by each model and determine whether there is a single model which could account for the observed data, or--more likely--whether there are still multiple models which could have generated the observed data.  

Approaches like this, employing information theoretic measures like AIC and BIC for model selection have existed for decades, and were popularized in ecology and the biological sciences by Burnham and Anderson's \citeyearpar{burnham2002model} influential book.\footnote{Today, given the immense rise in computing power available to the working scientist, summary statistic approaches like AIC are giving way to Bayesian approaches, whether fully Bayesian in the case of fully probabilistic models \citep{gelman2013bayesian,mcelreath2020statistical,robert1994bayesian}, or approximate estimation of the likelihood function by simulation \citep{Beaumont2002,Toni2009,Beaumont2010ur,Csillery:2010jd,Marin2011,Marin2012,sisson2018handbook}.}  With cultural transmission modeling in archaeological, approximate Bayesian methods will be standard, since the likelihood functions of our models will be analytical intractable.  Instead, simulation is used to create a large number of predicted summary statistics under a variety of parameter combinations specified as priors, and the distribution of these predicted summary statistics stands in as an approximate posterior.  Given the complexity of the models we seek to study in evolutionary archaeology, the ``generative approach'' as outlined by Kandler is destined to become virtually standard, in whatever kind of cultural transmission modeling we pursue in the future. 

\section{Are There Structural Equifinalities We Cannot Fix?}

Even with generative modeling and approximate Bayesian methods, it has proven difficult to distinguish between neutral and biased models of transmission \citep{kandler2019analysing}.  I believe this arises from several sources of real structural equifinality, that we cannot correct for, and which make the archaeological fitting of detailed cultural transmission models to individual populations a difficult or even impossible enterprise in most circumstances.  One cause arises from the nature of transmission and social learning in real populations; two further causes arise because of the stochastic nature of the models we necessarily employ and their interaction with the size of the ``state space'' (i.e., classification) employed, and our inability to sample multiple realizations of a stochastic process.

Real human populations, as we observe them today, are nowhere close to following any single ``mode'' or strategy for adopting cultural information and learning from their peers.  Realistic populations always include variation among individuals, and individuals may vary over time in the degree to which they exhibit conformist or novelty-seeking tendencies.  Individuals also may vary the learning strategies they employ depending upon the type of situation faced, or the kind of trait involved.  Thus, we should expect that populations will always be \emph{mixtures} of cultural transmission modes and learning strategies, and one would expect the statistical signatures of these strategies to present more complex statistical profiles.  In the worst cases, the contributions of different learning strategies and ``biases'' may even ``cancel out'' at the population level.  If this intuition about population mixtures is correct, then much of the equifinality we encounter in the microevolutionary approach is structural, and will not be resolved through better analytical methods---it is built into the phenomena themselves.

In Chapter \textcolor{red}{XX}, written in 2016, I examine the degree to which we can distinguish mixed populations, using a variation on the generative approach just described.  The study pairs agent-based simulations of differing population mixtures, comparing them to each other and to a population of unbiased copiers.  From each model in a pair, 23 different summary statistics are collected, all of which have been in use in the cultural transmission modeling literature.  The simulations incorporated the effects of time averaging and sample size, to determine the interaction between these critical empirical factors and our ability to cleanly separate models from summary data, by generating several different sets of "observed data" from each simulation pair with differing sample sizes and amounts of time averaging.  A state-of-the-art gradient boosting classifier determined the degree to which any combination of predictors were able to distinguish between models in a comparison, and which observable variables were important to separability and identifiability of the models \cite{AlexeyNatekin2013,Friedman2000,hastie2009elements}.  

The end result of the analysis is that equifinality is rife among mixture models.  When one can census the entire population and without any time averaging, it is typically possible to distinguish even the three-way mixture of anti-conformists, conformists, and some unbiased copiers from a pure population of unbiased copiers.  There is enough departure from the expectations of neutrality in that mixture population that the classifier can find combinations of predictors that separate the distributions.  But when one can only operate with finite samples, and when samples represent significant intervals of time, it becomes difficult or impossible to tell which mixture of models may be represented in empirical data.  Although the relationship between assemblage sample sizes and population sizes are typically not directly studied in most modeling exercises, our samples of the archaeological record cannot ever be more than samples of artifact discard and deposition from portions of a past population, and thus our inability to discriminate among mixtures of transmission modes seems structural and unavoidable.  

It is not hard to understand why this should be the case.  Our models of transmission are stochastic and incorporate chance both in the processes of learning between individuals, and in the ways we model innovation (although they make some good and insightful points, the general claims of \citealt{billiard2018stochasticity} that archaeologists have not incorporated stochasticity are exaggerated).  Biased and unbiased transmission models differ in their transition probabilities but otherwise share the same stochastic structure:  they are all Markov chains, without memory; they generally have quasi-stationary distributions and without innovation, absorbing or ``fixation'' states; and they all represent the state of the world as sequences of randomly chosen integer partitions \citep{crane2016ubiquitous}.  There is strong structural overlap in their distributional consequences, and the small departures from a power law distribution that might be diagnostic with a population census and many classes (e.g., baby names or dog breed frequencies) are going to be very hard to detect with small samples and small numbers of classes.  This is a direct consequence of the fact that the state space of the \emph{observed} process is much smaller than the state space of the original population, and the fact that the classifications used in many of these studies are relatively coarse-grained, so the number of classes or partitions is small (on the order of ten, not a hundred or a thousand).   The larger the number of partitions (or classes) represented, and the larger the sample size used, the larger the number of empirically distinguishable states.  Small numbers of classes and small sample sizes lead to small numbers of distinguishable states, relatively speaking.  The problem we face is that any of our microevolutionary models are often strongly overlapping in which distinguishable states they can assume; the difference is typically one of how probable any given state is from one model versus another, not its presence or absence in the solution set.  

This leads to a second source of structural equifinality.  When the behavior of transmission models is partially overlapping, it is very difficult to use a technique like approximate Bayesian computation to determine ``how likely'' it is for a single observation to be a realization of each model.  Typically, for confidence in determining the best model fit, one needs multiple samples from the population under study so that you can assess quantitatively which model is the most likely fit.  This means that the approach taken in some of the early, paradigm studies where a single assemblage or several components from a single site are compared to transmission models, will tend not to have well-defined results.  Kandler's \citeyear{Kandler2013} diachronic, ``non-equilibrium'' approach does help by looking at the likelihood of \emph{trajectories} of observations, not single data points, but this only becomes powerful in the face of sample size and time averaging issues if one could sample enough points over time and space.  

And that, I believe, is the real solution.  The predictions we make from transmission models need to be richer---predictions that encompass the quantitative aspects of information flow at more than just a couple of points in time and at more than one location.  Richer predictions would be compared to richer observables:  structures which can represent diachronic change, as well as spatial variation, and which can vary in enough ways even with typical archaeological sample sizes that we can distinguish between hypotheses about evolutionary history.  

\section{Seriation and the Mesoscopic Approach to Cultural Transmission Modeling}

One of the reasons Carl Lipo and I \citep{huntmadsenlipo1995a,lipomadsenhunt1995b,Lipo1997,Lipo2000,lipomadsen1997,Lipo2001a,Lipo2001} began systematically exploring seriations as observable units for fitting transmission models to archaeological data was that they automatically incorporate the diachronic nature of our data, incorporating finite durations for each of the assemblages that make up the seriation \citep{dunnell1970seriation}.  In some of our early work on the subject, we introduced an iterative method for finding deterministic solutions to the seriation problem, by partitioning the full set of assemblages into subsets, where each subset fully meets the requirements for unimodality \citep{Lipo1997}.  Carl Lipo extended this work in his own dissertation, by calculating bootstrap confidence intervals around observed class frequencies, which allows one to assess the unimodality criterion for a set of assemblages while taking into account the likely effects of sampling error \citep{Lipo2001}.  

The conversion of seriation solutions from a single linear ordering for all assemblages (as had been the practice virtually since 1916) to multiple subsolutions accomplishes two important results.  First, it ensures that each subset actually meets the seriation criterion being used (e.g., unimodality or continuity), and second, it directly incorporates the spatial variation present in the history of artifact classes across a region.  One of the key reasons why classical seriations used a ``same local area'' criterion to \emph{limit} the amount of spatial variation one put into a seriation is that different places lead to different orders, and it is impossible to accommodate these different histories in a single linear order.  The answer is simply to break them out into different solutions, each of which tells you something unique about the history of artifact assemblages in specific places and times.  In doing this kind of multiple solution technique, it is important that individual assemblages be allowed within multiple subsets.  This provides a way to understand the way in which information may be differentially flowing in and out of localities within a region, over time.  In practice, a small subset of assemblages do tend to occur in multiple subsets, as Lipo found in his seriations of Mississippian ceramics from the central and lower Mississippi River valley.  

Much of this early work was still accomplished by manual assortment of assemblages and then software-based confirmation of the significance of candidate solutions via bootstrap testing (using an Excel macro package which still gets download requests today).  More recently, Lipo and I \citeyearpar{Lipo2015}, finishing work begun with R.C. Dunnell before his passing, automated the process of finding multiple-subset seriation solutions, by converting the problem to one of graph or tree construction.  In that work, which is not included in this dissertation, we outlined an approach to seriation graph construction that employs the bootstrap confidence interval testing and employs heuristics to quickly prune the set of possible solutions.  This helps avoid, but cannot prevent, the combinatorial explosion that occurs as the number of assemblages increases.  In a short research note, included here as Chapter \textcolor{red}{YY}, I examine how employing multiple solution groups affects the size of the ``solution space'' for the seriation problem and indeed \emph{increases} the number of possible solutions for a given set of assemblages by orders of magnitude over the permutations available in a straight linear order.

This sounds like a bad thing, and without good heuristics on finding possible linkages within the candidate solutions it would be.  But in practice, the ``iterative deterministic seriation solution'' (IDSS) method has proven relatively tractable with around a dozen assemblages, especially with the significant increases in computing power now available to researchers.  But it is well worth looking at how to efficiently find solutions when we have 20, 30, or 50 assemblages.  Why?  Because the larger the set of assemblages we can include, with their variation in artifact class frequencies, the more data we are sampling from the single realization which was the actual history of cultural information in some chunk of the past.  

In order to improve our efficiency in finding complex solution graphs, I worked with Carl Lipo to examine alternate criteria for forming solutions.  There is nothing special about unimodality in a seriation.  As Fraser Neiman documented in his dissertation \citeyearpar{Neiman1990}, realizations of an unbiased transmission model like Wright-Fisher easily give rise to unimodal rises in the ``popularity'' of some trait, peak, and then decline, just as \citet{Nelson1916} and Wissler \citet{wissler1916application} described in the earliest recognizable ``frequency seriations.''  But transmission gives rise to other patterns, and can easily give rise to multimodal distributions for a trait over time as well (some things come back into ``fashion'', as we well know from contemporary life).  Unimodality is not the default criterion for frequency seriation because it's the only pattern possible, but because it is a distinctive pattern among many that can be used to ``tell time'', or in our terms, to find the unique history of information sharing between and among communities.  Recognizing this, we can look for other ways of doing frequency seriation that yield equivalent results, but are more general and more efficient.  

Chapter \textcolor{red}{YX} describes our work comparing unimodality to other possible ordering algorithms, and in particular distance-minimization, building upon Kadane's \citeyearpar{Kadane1971} earlier work.  The principle is to find seriation solution graphs which globally minimize the total amount of change between neighboring pairs of assemblages.  Because this method alludes to ideas of mathematical ``smoothness'' and ``continuity,'' we dubbed the method ``continuity'' seriation.  With continuity seriation, the efficiency of the calculation dramatically increases the size of data sets that can be analyzed, by providing a roughly 25x speedup in evaluating solutions (conservatively).   The expansion of the size of the solution space is a \emph{good} thing when we think about cultural transmission models and their possible outcomes over many assemblages regionally.  If we treat a seriation graph on a set of assemblages as a \emph{realization of cultural transmission outcomes within a region over time}, it is likely that some models of regional transmission are compatible with that realization (seriation) and many will not be. 

In this chapter as well, I outline a generative approach to using the resulting seriations to perform model selection among different candidate ``transmission scenarios'' that could account for them.  A transmission scenario is defined as a candidate regional history of the social network between communities and how it might change over time.  The \emph{results} of repeated acts of cultural transmission over this social network results in differential adoption and persistence of stylistic or neutral-behaving artifact classes (sensu \citealt{Dunnell1978}) across the whole set.  Standard social network models are again, synchronic snapshots, so the formalism adopted here is that of an ``interval temporal network'' to model how connectivity changes over time \citep{Holme2012}.  Since we cannot really tell the difference between most kinds of biased transmission and unbiased copying at this scale (as I argue in this dissertation), information flow on an interval temporal network is simply modeled as a standard Wright-Fisher process, albeit on a metapopulation network which changes over time.  The overall structure of the modeling exercise is as follows:

\begin{itemize}
    \item Construct candidate spatiotemporal network models to represent scenarios of interest
    \item Simulate a large number of replicates of unbiased cultural transmission on each of the generated spatiotemporal network models
    \item Take samples from each simulation realization, perform time averaging according to the durations of nodes as given by the temporal network model
    \item Treat the results like archaeological samples, eliminate unique traits that are not shared across assemblages, and find the best seriation solution for the sampled frequency data
    \item Analyze the ``shape'' or topology of the resulting seriation graphs to extract statistics about their structure
    \item Use a classifier to determine the degree to which the seriations cleanly identify different transmission scenarios
\end{itemize}

This is a generative modeling exercise, with the same structure as Kandler's \citeyearpar{Kandler20150905} method, but at a different scale of analysis.  This is clearly no longer a ``microevolutionary'' question, about what transmission processes may account for the assemblage data we see in a particular time and place.  It is a ``mesoscopic'' question, sitting between the details of a single population over a short term interval, and the relations of a region with other regions as we often see in macroevolutionary studies.  But a ``mesoscopic'' approach does not mean we cannot ask questions about how cultural transmission might be structured, or even biased at the large scale.  

In the same chapter, I report the results of examining four different regional scenarios:  complete networks, where all communities communicate in roughly the same proportion, transmission with a ``nearest neighbor'' bias, in which much smaller numbers of long distance links occur in the social network, a model where a complete network splits into two or more ``lineages'' with less sharing between subsequent to the split, and a ``lineage coalescence'' model where formerly separate lineages begin to form a larger metapopulation with more complete communication.  In general the results seem hopeful.  Using the eigenvalues of the Laplacian matrix of seriation solution graphs as the input variables to a standard gradient boosted tree classifier, we are able to tell the difference between lineage splitting, nearest neighbor, and more homogeneous social networks.  Other comparisons are not clearly identifiable, which is something we should expect, and I outline some of the limitations seen so far.  Finally, using the trained gradient boosting classifier, I was able to make predictions about which model best fit our seriation results from the Late Prehistoric ceramic data in the central Mississippi River valley, and it appears that the results are consistent with a lineage splitting event.  This is consistent with the overall archaeological evidence for this data set \citep{Lipo2001}.  This kind of generative approach with seriation graphs as the unit of observation has considerable promise, and this initial work is simply a downpayment on exploring issues such as the sample sizes needed to resolve different classes of regional scenarios.  

\section{Dependency Graphs and Incorporating Structured Information In Cultural Transmission Studies}

Finally, it is not enough to understand the flow of cultural information through space and time, we need methods to understand how the \emph{content} of culturally transmitted information changes over time, and how the kind of information may affect its transmission.  Simple cultural transmission models tend to treat cultural traits in the manner of ``bean bag genetics'' -- as markers which come and go and are subject to innovation, but have little structure among themselves.  Cultural information and the skills that people inherit and pass on with that information are nothing like simple markers.  William Wimsatt \citep{wimsatt2007reproducing,wimsatt2013articulating,Wimsatt2014,wimsatt2019articulating} has made this the central focus of his work on cultural evolution, bringing together researchers with a variety of disciplinary foci to make ``development'' central to the study of cultural transmission.  Within archaeology, Mesoudi and O'Brien \citep{mesoudi2008learning} described the importance of modeling the structured relationships between cultural traits, and \citet{tostevin2019content} has richly developed this idea, combining it with Wimsatt's idea that some cultural traits provide ``scaffolding'' needed to learn others.  Tostevin in particular is attempting to create ``thick descriptions'' of how social learning and cultural transmission articulate with the actual physical process of flintknapping and learning to flintknap, such that we can articulate the actual physics of the technology with the homologies we see over longer spans of time as methods are taught and learned.  

This kind of fruitful marriage between social learning theory, the details of specific technologies, and longer term patterns in transmission is exciting and represents moving beyond simple models to explanatory models which will use the idea of cultural transmission to answer real questions about our evolutionary history.  But to get there, we need to understand what kinds of tools we need to make the articulation.  What kinds of observable units, and what kinds of quantitative features on those observables, will let us differentiate between hypotheses?  My initial attempt to tackle this question is contained in the final paper presented in this dissertation, written in 2015 for a volume on social learning in Neandertals and early modern humans.  

In this work, I examine one specific class of ``structured information'' relationships:  the idea that traits may have prerequisites.  Some alternatives, some skills, some variants on our tools, may not be things we can learn about unless we've mastered other things, first.  We can represent these relationships among cultural traits or artifact classes in the abstract by dependency trees; nodes higher in the tree are prerequisites for things lower down.  With such a model of dependencies, I studied how different learning models (individual trial and error versus targeted teaching by a peer) led to cultural repertoires of different structure and richness.  In common with the theme of much of my recent work, I employed a simulation approach and examined the ``knowledge graphs'' that simulated individuals have after many rounds of transmission conditioned by different rates of teaching and individual innovation.  Since the dependency structures and traits are abstract in this kind of study, the observable are trees of traits, and I analyzed their topology and symmetries to determine how deep, how shallow, how ``broad'' versus deep and narrow, etc.  The study, conducted in the context of a symposium on the ``learning hypothesis'' for behavioral modernity in the Upper Paleolithic \citep{Nishiaki2013}, supports the hypothesis that the evolution of mechanisms of teaching and apprenticeship would lead to enriched cultural repertoires and growth in cultural diversity.

Within the context of this dissertation, this study is significant because it demonstrates, once again, that studying cultural transmission within archaeology will require careful consideration of \emph{how we structure our observations}.  The observational units we employ are not a given---just because we have artifact class frequencies from our tabulations of classifications built for one purpose does not mean those data will be informative for another purpose, as Dunnell spent a lifetime reminding us \citep{Dunnell1971,dunnell1986methodological}.  We must build observational units using a combination of good old fashioned artifact classification, and all of the mathematical, statistical, and machine learning sophistication we can muster, if we are going to use cultural transmission models fruitfully in archaeology to tackle questions of evolutionary history.  And we must tackle questions of evolutionary history if we are going to avail ourselves of the bodies of theoretical machinery, from evolutionary game theory to decision-theoretic modeling, to form complete evolutionary explanations \citeeg{gintis2014bounds}.